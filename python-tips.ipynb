{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Lambda, map, filter, reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triple_fn(x):\n",
    "    return x * x * x\n",
    "\n",
    "triple_ld = lambda x: x * x * x\n",
    "\n",
    "for n in range(10):\n",
    "    assert triple_fn(n) == triple_ld(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lambda keyword is used to created inline functions. The functions `triple_fn` and `triple_ld` above are identical. The `lambda` functions is ideal for use in callbacks, as well as when functions are to be passed as arguments to other functions, e.g. when use in conjunction with functions like `maps`, `filter` and `reduce`.\n",
    "\n",
    "`map(fn, iterable)` applies the `fn` to all elements of the `iterable`, e.g. list, set, dictionary, tuple and string, and returns a map object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.125, 11390.625, 4.295269396182883, 11981.247506615104, 0.3644314868804665, 41849.68966674805]\n"
     ]
    }
   ],
   "source": [
    "seqs = [1/2, 90/4, 5678/3493, 778/34, 5/7, 9999/288]\n",
    "seqs_tripled = [seq * seq * seq for seq in seqs]\n",
    "print(f'{seqs_tripled}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same when using `map` with a callback function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.125, 11390.625, 4.295269396182883, 11981.247506615104, 0.3644314868804665, 41849.68966674805]\n"
     ]
    }
   ],
   "source": [
    "seqs_tripled_with_fn = map(triple_fn, seqs)\n",
    "seqs_tripled_with_ld = map(lambda x: x * x * x, seqs)\n",
    "\n",
    "print(f'{list(seqs_tripled_with_fn)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.125, 11390.625, 4.295269396182883, 11981.247506615104, 0.3644314868804665, 41849.68966674805]\n"
     ]
    }
   ],
   "source": [
    "print(f'{list(seqs_tripled_with_ld)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use `map` with more than one iterable, e.g. if you want to calculate the mean squared error of a simple linear function `f(x) = ax + b` with the true label labels, these two methods are equivalent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.35089172119045514, 0.35089172119045514)\n"
     ]
    }
   ],
   "source": [
    "a, b = 3, -0.5\n",
    "xs = [2, 3, 4, 5]\n",
    "labels = [6.4, 8.9, 10.9, 15.3]\n",
    "\n",
    "# Method 1: using a loop\n",
    "errors = []\n",
    "for i, x in enumerate(xs):\n",
    "    errors.append((a * x + b - labels[i]) ** 2)\n",
    "result1 = sum(errors) ** 0.5 / len(xs)\n",
    "\n",
    "# Method 2: using map\n",
    "diffs = map(lambda x, y: (a * x + b - y) ** 2, xs, labels)\n",
    "result2 = sum(diffs) ** 0.5 / len(xs)\n",
    "\n",
    "print(f'{result1, result2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that objects returned by `map` and `filter` are iterators, which means that their values aren't stored but generated as needed. After you've called `sum(diffs)`, `diffs` becomes empty. If you want to keep all elements in `diffs`, convert it to a list using `list(diffs)`.\n",
    "\n",
    "`filter(fn, iterable)` works the same way as `map`, except that `fn` returns a boolean value and `filter` returns all the elements of the `iterable` for which the `fn` returns True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8100000000000006, 0.6400000000000011]\n"
     ]
    }
   ],
   "source": [
    "not_nice_prediction = filter(lambda x: x > 0.5, errors)\n",
    "print(f'{list(not_nice_prediction)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`reduce(fn, iterable, initializer)` is used when we want to iteratively apply an operator to all elements in a list. For example, if we want to calculate the product of all elements in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10377.34009153511\n"
     ]
    }
   ],
   "source": [
    "product = 1\n",
    "for seq in seqs:\n",
    "    product *= seq\n",
    "\n",
    "print(f'{product}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is equivalent to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10377.34009153511\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "product = reduce(lambda x, y: x * y, seqs)\n",
    "print(f'{product}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on the performance of lambda functions\n",
    "Lambda functions are meant for one time use. Each time `lambda x: dosomething(x)` is called, the function has to be created, which hurts the performance if you call `lambda x: dosomething(x)` multiple times, e.g. when you pass it inside `reduce`.\n",
    "\n",
    "When you assign a name to the lambda function as in `fn = lambda x: dosomething(x)`, its performance is slightly slower than the same function defined using `def`, but the difference is negligible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 List manipulation\n",
    "\n",
    "## 2.1 Unpacking\n",
    "Unpacking can be done like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8]\n"
     ]
    }
   ],
   "source": [
    "nums = [2, 4, 6, 8]\n",
    "a, b, c, d = nums\n",
    "print(nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can also be unpacked a list like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[4, 6]\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "a, *new_nums, d = nums\n",
    "print(f'{a}')\n",
    "print(f'{new_nums}')\n",
    "print(f'{d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Slicing\n",
    "We can reverse a list like using `[::-1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "nums = list(range(10))\n",
    "print(f'{nums}')\n",
    "\n",
    "print(f'{nums[::-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The syntax `[x:y:z]` means \"take every `z`th element of a list from index `x` to index `y`\". When `z` is negative, it indicates going backwards. When `x` isn't specified, it defaults to the first element of the list in the direction you are traversing the list. When `y` isn't specified, it defaults to the last element of the list. So if we want to take every 2th element of a list, we use `[::2]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 4, 6, 8]\n",
      "[8, 6, 4, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "evens = nums[::2]\n",
    "print(f'{evens}')\n",
    "\n",
    "reversed_evens = nums[-2::-2]\n",
    "print(f'{reversed_evens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also can use slicing to delete ll the even numbers in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5, 7, 9]\n"
     ]
    }
   ],
   "source": [
    "del nums[::2]\n",
    "print(f'{nums}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Flatttening\n",
    "A list of lists can be flattened using `sum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_lists = [[1], [2, 3], [4, 5, 6]]\n",
    "sum(list_of_lists, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For nested lists, we can recursively flatten it, through together with the lambda functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nested_lists = [[1, 2], [[3, 4], [5, 6], [[7, 8], [9, 10], [[11, [12, 13]]]]]]\n",
    "flatten = lambda x: [y for l in x for y in flatten(l)] if type(x) is list else [x]\n",
    "flatten(nested_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 List vs generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['we', 'have', 'a'], ['have', 'a', 'nice'], ['a', 'nice', 'weather'], ['nice', 'weather', 'today']]\n"
     ]
    }
   ],
   "source": [
    "tokens = ['we', 'have', 'a', 'nice', 'weather', 'today']\n",
    "\n",
    "def ngrams(token, n):\n",
    "    length = len(tokens)\n",
    "    grams = []\n",
    "    for i in range(length - n + 1):\n",
    "        grams.append(tokens[i:i+n])\n",
    "    \n",
    "    return grams\n",
    "\n",
    "print(f'{ngrams(tokens, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we have to store all the n-grams at the same time. If the text has m tokens, then the memory requirement is `O(nm)`, which can be problematic when m is large\n",
    "\n",
    "Instead of using a list to store all n-grams, we can use a generator that generates the next n-gram when it's asked for. This is known as lazy evaluation. We can make the function `ngrams` returns a generator using the keyword `yield`. Then the memory requirement is `O(m+n)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object ngrams at 0x7ff417d9b200>\n",
      "['we', 'have', 'a']\n",
      "['have', 'a', 'nice']\n",
      "['a', 'nice', 'weather']\n",
      "['nice', 'weather', 'today']\n"
     ]
    }
   ],
   "source": [
    "def ngrams(tokens, n):\n",
    "    length = len(tokens)\n",
    "    for i in range(length - n + 1):\n",
    "        yield tokens[i:i+n]\n",
    "\n",
    "ngrams_generator = ngrams(tokens, 3)\n",
    "print(f'{ngrams_generator}')\n",
    "\n",
    "for ngram in ngrams_generator:\n",
    "    print(f'{ngram}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option to generate n-grams is to use slices to create lists: `[0, 1, ..., -n]`, `[1, 2, ..., -n+1]`, ..., `[n-1, n, ..., -1]`, and then zip them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function ngrams at 0x7ff4181859d0>\n",
      "('we', 'have', 'a')\n",
      "('have', 'a', 'nice')\n",
      "('a', 'nice', 'weather')\n",
      "('nice', 'weather', 'today')\n"
     ]
    }
   ],
   "source": [
    "def ngrams(tokens, n):\n",
    "    length = len(tokens)\n",
    "    slices = (tokens[i:length-n+i+1] for i in range(n))\n",
    "    return zip(*slices)\n",
    "\n",
    "ngrams_generator = ngrams(tokens, 3)\n",
    "print(f'{ngrams}')\n",
    "\n",
    "\n",
    "for ngram in ngrams_generator:\n",
    "    print(f'{ngram}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create slices, we use `(tokens[...] for i in range(n))` instead of `[tokens[...] for i in range(n)]`. `[]` is the normal list comprehension that returns a list. `()` returns a generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 local namespace, object's attributes\n",
    "The `locals()` function returns a dictionary containing the variables defined in the local namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'self': <__main__.Model1 object at 0x7ff417dad4f0>, 'hidden_size': 10, 'num_of_layers': 2, 'learning_rate': 2}\n"
     ]
    }
   ],
   "source": [
    "class Model1:\n",
    "    def __init__(self, hidden_size=10, num_of_layers=2, learning_rate=2):\n",
    "        print(f'{locals()}')\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_of_layers = num_of_layers\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "model1 = Model1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All attributes of an object are stored in its `__dict__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_size': 10, 'num_of_layers': 2, 'learning_rate': 2}\n"
     ]
    }
   ],
   "source": [
    "print(f'{model1.__dict__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To manually assign each of the arguments to an attribute can be quite tiring when the list of the arguments is large. To avoid this, we can directly assign the list of arguments to the object's `__dict__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_size': 10, 'num_of_layers': 2, 'learning_rate': 2}\n"
     ]
    }
   ],
   "source": [
    "class Model2:\n",
    "    def __init__(self, hidden_size=10, num_of_layers=2, learning_rate=2):\n",
    "        params = locals()\n",
    "        del params['self']\n",
    "        self.__dict__ = params\n",
    "\n",
    "model2 = Model2()\n",
    "print(f'{model2.__dict__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be especially convenient when the object is initiated using the catch-all `**kwargs`, though the use of `**kwargs` should be reduced to the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_size': 10, 'num_layers': 2, 'learning_rate': 2}\n"
     ]
    }
   ],
   "source": [
    "class Model3:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__ = kwargs\n",
    "\n",
    "model3 = Model3(hidden_size=10, num_layers=2, learning_rate=2)\n",
    "print(f'{model3.__dict__}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Wild import\n",
    "\n",
    "Often, you run into this wild import `*` that looks something like this:\n",
    "\n",
    "`file.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parts import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will import everything in the module, even the imports of the module, which prove may not be a good idea. Alternatively, if we intend that only ClassA, ClassB, and MethodA are ever to be imported and used in another module, we should specify that in parts.py using the __all__ keyword.\n",
    "\n",
    "`parts.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " __all__ = ['ClassA', 'ClassB', 'MethodA']\n",
    "import numpy\n",
    "import tensorflow\n",
    "\n",
    "class ClassA:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if someone does a wild import with `parts`, they can only import `ClassA`, `ClassB`, and `MethodA`. Personally, I also find __all__ helpful as it gives me an overview of the module"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-python-libraries-new",
   "language": "python",
   "name": "ml-python-libraries"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
